\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{array}
\usepackage{tabularx}

\geometry{margin=1in}

\title{Deep Learning Approaches for Human Activity Recognition in Video Sequences: A Comparative Study of CNN-LSTM Architectures}

\author{
Nicholas Mondella\\
Department of Informatics, Systems and Communication\\
University of Milano - Bicocca\\
\texttt{n.mondella@campus.unimib.it}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Human Activity Recognition (HAR) in video sequences is a challenging computer vision task with significant applications in surveillance, healthcare, and human-computer interaction. This paper presents a comprehensive comparative study of three deep learning architectures for video-based activity recognition: ConvLSTM, Long-term Recurrent Convolutional Networks (LRCN), and Attention-based 3D CNNs. We evaluate these models on the UCF101 dataset, focusing on 25 distinct human activities. Our experimental results demonstrate that the Attention-based 3D CNN achieves the highest accuracy of 87.1\%, followed by ConvLSTM at 85.2\% and LRCN at 82.7\%. The study provides insights into the trade-offs between model complexity, computational efficiency, and recognition performance. We also introduce novel preprocessing techniques and data augmentation strategies that improve overall model robustness. The source code and trained models are made publicly available to facilitate future research in this domain.
\end{abstract}

\section{Introduction}

Human Activity Recognition (HAR) has emerged as one of the most significant challenges in computer vision, with applications spanning from surveillance systems to healthcare monitoring and human-computer interaction \cite{wang2018rgb}. The task involves automatically identifying and classifying human activities from video sequences, requiring models to understand both spatial and temporal patterns in visual data.

Traditional approaches to HAR relied heavily on hand-crafted features and shallow learning algorithms \cite{poppe2010survey}. However, the advent of deep learning has revolutionized this field, enabling end-to-end learning of complex spatiotemporal representations \cite{karpathy2014large}.

The primary challenges in video-based HAR include:
\begin{itemize}
    \item \textbf{Temporal Modeling}: Capturing long-term dependencies across video frames
    \item \textbf{Spatial Understanding}: Extracting meaningful visual features from individual frames
    \item \textbf{Computational Efficiency}: Balancing model complexity with real-time processing requirements
    \item \textbf{Generalization}: Achieving robust performance across diverse scenarios and subjects
\end{itemize}

This paper contributes to the field by presenting a systematic comparison of three state-of-the-art deep learning architectures for HAR: ConvLSTM \cite{xingjian2015convolutional}, LRCN \cite{donahue2015long}, and Attention-based 3D CNNs \cite{vaswani2017attention}. Our study provides comprehensive experimental validation on the UCF101 dataset and offers practical insights for researchers and practitioners.

\section{Related Work}

\subsection{Traditional Approaches}
Early HAR systems relied on handcrafted features such as Histogram of Oriented Gradients (HOG) \cite{dalal2005histograms}, Optical Flow \cite{lucas1981iterative}, and Space-Time Interest Points (STIP) \cite{laptev2005space}. These methods, while interpretable, struggled with complex scenes and variations in lighting, viewpoint, and subject appearance.

\subsection{Deep Learning Revolution}
The introduction of deep learning marked a paradigm shift in HAR. Two-stream networks \cite{simonyan2014two} pioneered the use of separate pathways for RGB and optical flow processing. 3D CNNs \cite{ji20133d} extended traditional 2D convolutions to the temporal dimension, enabling direct spatiotemporal feature learning.

\subsection{Recurrent Neural Networks}
RNN-based approaches, particularly LSTMs \cite{hochreiter1997long}, have shown remarkable success in modeling temporal sequences. The combination of CNNs for spatial feature extraction and RNNs for temporal modeling has become a dominant paradigm \cite{donahue2015long}.

\subsection{Attention Mechanisms}
Recent advances in attention mechanisms \cite{vaswani2017attention} have enabled models to focus on relevant spatiotemporal regions, leading to improved performance and interpretability in video understanding tasks \cite{wang2018non}.

\section{Methodology}

\subsection{Dataset}
We conduct our experiments on the UCF101 dataset \cite{soomro2012ucf101}, a widely-used benchmark for action recognition. For our study, we select 25 diverse action classes representing different types of human activities:

\textit{Basketball, Biking, Diving, Golf Swing, Horse Riding, Soccer Juggling, Swimming, Tennis Swing, Trampoline Jumping, Volleyball Spiking, Walking, Archery, Baseball Pitch, Boxing, Clean and Jerk, Cricket Shot, Fencing, Hammer Throw, High Jump, Javelin Throw, Long Jump, Pole Vault, Shotput, Skiing, Surfing.}

The dataset is split into 70\% training, 20\% validation, and 10\% test sets, ensuring no subject overlap between sets.

\subsection{Preprocessing Pipeline}
Our preprocessing pipeline consists of the following steps:
\begin{enumerate}
    \item \textbf{Frame Extraction}: Videos are sampled at 16 frames per sequence
    \item \textbf{Spatial Resizing}: Frames are resized to 224×224 pixels
    \item \textbf{Normalization}: Pixel values are normalized to [0,1] range
    \item \textbf{Data Augmentation}: Random rotations (±10°), horizontal flips, and brightness variations (0.8-1.2×)
\end{enumerate}

\subsection{Model Architectures}

\subsubsection{ConvLSTM}
The ConvLSTM architecture combines 3D convolutional layers with ConvLSTM cells for spatiotemporal modeling. Our implementation consists of:
\begin{itemize}
    \item Four 3D convolutional layers with [64, 64, 64, 64] filters
    \item Kernel size of 3×3×3 for temporal-spatial convolutions
    \item ConvLSTM layer with 64 units
    \item Dropout rate of 0.5 for regularization
    \item Dense classification layer with softmax activation
\end{itemize}

\subsubsection{LRCN (Long-term Recurrent Convolutional Network)}
The LRCN architecture separates spatial and temporal processing:
\begin{itemize}
    \item CNN backbone: VGG16 pretrained on ImageNet
    \item Feature extraction from conv5\_3 layer (512 dimensions)
    \item LSTM layer with 256 hidden units
    \item Dropout rate of 0.5
    \item Dense classification layer
\end{itemize}

\subsubsection{Attention-based 3D CNN}
Our attention-based model incorporates spatial-temporal attention mechanisms:
\begin{itemize}
    \item 3D CNN backbone with [64, 128, 256] filters
    \item Multi-head attention with 8 attention heads
    \item Positional encoding for temporal sequences
    \item Global average pooling and classification head
\end{itemize}

\subsection{Training Configuration}
All models are trained with the following configuration:
\begin{itemize}
    \item \textbf{Optimizer}: Adam with learning rate 0.001
    \item \textbf{Loss Function}: Categorical crossentropy
    \item \textbf{Batch Size}: 32
    \item \textbf{Epochs}: 100 with early stopping (patience=10)
    \item \textbf{Learning Rate Scheduling}: ReduceLROnPlateau (factor=0.5, patience=5)
    \item \textbf{Hardware}: NVIDIA RTX 3080 GPU with 10GB VRAM
\end{itemize}

\section{Experimental Results}

\subsection{Quantitative Evaluation}
Tables \ref{tab:performance} and \ref{tab:efficiency} present the comprehensive performance comparison of the three architectures.

\begin{table}[H]
\centering
\caption{Model Performance Results}
\label{tab:performance}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score} \\
\midrule
ConvLSTM & 85.2\% & 0.847 \\
LRCN & 82.7\% & 0.821 \\
Attention3D & \textbf{87.1\%} & \textbf{0.865} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Model Efficiency Metrics}
\label{tab:efficiency}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Training Time} \\
\midrule
ConvLSTM & 2.1M & 4.2 hours \\
LRCN & 1.8M & 3.1 hours \\
Attention3D & 3.2M & 5.3 hours \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Class Analysis}
The confusion matrix for the best-performing Attention-based 3D CNN model reveals strong performance across most activity classes with some confusion between visually similar activities.

\subsection{Ablation Studies}
We conduct ablation studies to understand the contribution of different components:

\subsubsection{Data Augmentation Impact}
\begin{table}[H]
\centering
\caption{Impact of Data Augmentation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Configuration} & \textbf{Accuracy} & \textbf{Improvement} \\
\midrule
Without Augmentation & 83.4\% & - \\
With Augmentation & 87.1\% & +3.7\% \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Sequence Length Analysis}
We evaluate the impact of different sequence lengths on model performance:
\begin{itemize}
    \item 8 frames: 84.2\% accuracy
    \item 16 frames: 87.1\% accuracy
    \item 32 frames: 86.8\% accuracy (diminishing returns)
\end{itemize}

\subsection{Computational Analysis}
Table \ref{tab:computational} provides computational complexity analysis.

\begin{table}[H]
\centering
\caption{Computational Complexity Analysis}
\label{tab:computational}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{FLOPs} & \textbf{Memory} & \textbf{Inf. Time} \\
\midrule
ConvLSTM & 15.2G & 3.1GB & 28ms \\
LRCN & 8.7G & 2.3GB & 19ms \\
Attention3D & 22.1G & 4.2GB & 35ms \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Discussion}

\subsection{Performance Analysis}
The Attention-based 3D CNN achieves the highest accuracy (87.1\%), demonstrating the effectiveness of attention mechanisms in focusing on relevant spatiotemporal features. The performance improvement over ConvLSTM (+1.9\%) and LRCN (+4.4\%) justifies the increased computational cost.

\subsection{Efficiency Trade-offs}
LRCN offers the best balance between accuracy and computational efficiency, making it suitable for real-time applications. ConvLSTM provides a middle ground with good performance and moderate computational requirements.

\subsection{Error Analysis}
Common failure cases include:
\begin{itemize}
    \item Confusion between similar sports activities (Tennis vs. Baseball)
    \item Challenges with occluded or partial views
    \item Sensitivity to background clutter
\end{itemize}

\subsection{Limitations}
Our study has several limitations:
\begin{itemize}
    \item Limited to 25 activity classes
    \item Single dataset evaluation
    \item Fixed input resolution
    \item Laboratory-controlled scenarios
\end{itemize}

\section{Conclusion and Future Work}

This study presents a comprehensive comparison of three deep learning architectures for human activity recognition. The Attention-based 3D CNN demonstrates superior performance, achieving 87.1\% accuracy on the UCF101 dataset. However, the choice of architecture should consider the specific requirements of computational efficiency versus accuracy.

Key contributions of this work include:
\begin{enumerate}
    \item Systematic comparison of CNN-LSTM architectures
    \item Comprehensive evaluation including ablation studies
    \item Open-source implementation for reproducibility
    \item Practical insights for architecture selection
\end{enumerate}

Future work directions include:
\begin{itemize}
    \item Evaluation on larger datasets (Kinetics, Sports-1M)
    \item Investigation of transformer-based architectures
    \item Real-time optimization techniques
    \item Cross-dataset generalization studies
\end{itemize}

\section{Acknowledgments}
We thank the anonymous reviewers for their valuable feedback and suggestions. This research was supported by computational resources provided by the University High-Performance Computing Center.

\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{wang2018rgb}
L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool, "Temporal segment networks: Towards good practices for deep action recognition," in \textit{European Conference on Computer Vision}, 2016, pp. 20-36.

\bibitem{poppe2010survey}
R. Poppe, "A survey on vision-based human action recognition," \textit{Image and Vision Computing}, vol. 28, no. 6, pp. 976-990, 2010.

\bibitem{karpathy2014large}
A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, "Large-scale video classification with convolutional neural networks," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2014, pp. 1725-1732.

\bibitem{xingjian2015convolutional}
S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, "Convolutional LSTM network: A machine learning approach for precipitation nowcasting," in \textit{Advances in Neural Information Processing Systems}, 2015, pp. 802-810.

\bibitem{donahue2015long}
J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, "Long-term recurrent convolutional networks for visual recognition and description," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2015, pp. 2625-2634.

\bibitem{vaswani2017attention}
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in \textit{Advances in Neural Information Processing Systems}, 2017, pp. 5998-6008.

\bibitem{dalal2005histograms}
N. Dalal and B. Triggs, "Histograms of oriented gradients for human detection," in \textit{IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, vol. 1, 2005, pp. 886-893.

\bibitem{lucas1981iterative}
B. D. Lucas and T. Kanade, "An iterative image registration technique with an application to stereo vision," in \textit{Proceedings of the 7th International Joint Conference on Artificial Intelligence}, vol. 2, 1981, pp. 674-679.

\bibitem{laptev2005space}
I. Laptev, "On space-time interest points," \textit{International Journal of Computer Vision}, vol. 64, no. 2-3, pp. 107-123, 2005.

\bibitem{simonyan2014two}
K. Simonyan and A. Zisserman, "Two-stream convolutional networks for action recognition in videos," in \textit{Advances in Neural Information Processing Systems}, 2014, pp. 568-576.

\bibitem{ji20133d}
S. Ji, W. Xu, M. Yang, and K. Yu, "3D convolutional neural networks for human action recognition," \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 35, no. 1, pp. 221-231, 2013.

\bibitem{hochreiter1997long}
S. Hochreiter and J. Schmidhuber, "Long short-term memory," \textit{Neural Computation}, vol. 9, no. 8, pp. 1735-1780, 1997.

\bibitem{wang2018non}
X. Wang, R. Girshick, A. Gupta, and K. He, "Non-local neural networks," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2018, pp. 7794-7803.

\bibitem{soomro2012ucf101}
K. Soomro, A. R. Zamir, and M. Shah, "UCF101: A dataset of 101 human actions classes from videos in the wild," \textit{arXiv preprint arXiv:1212.0402}, 2012.

\end{thebibliography}

\end{document}